# --------------------------------------------------------------------------
# Jesus Tordesillas Torres, Robotic Systems Lab, ETH ZÃ¼rich
# See LICENSE file for the license information
# --------------------------------------------------------------------------

import torch
import torch.nn as nn
from . import utils
import numpy as np
import cvxpy as cp
import math
from cvxpylayers.torch import CvxpyLayer
import random
import copy
import time


class ConstraintModule(torch.nn.Module):
    def __init__(
        self,
        cs,
        input_dim=None,
        method="RAYEN",
        create_map=True,
        args_DC3=None,
    ):
        super().__init__()

        self.method = method

        if self.method == "Bar" and cs.has_quadratic_constraints:
            raise Exception(
                f"Method {self.method} cannot be used with quadratic constraints"
            )

        if self.method == "DC3" and (cs.has_soc_constraints or cs.has_lmi_constraints):
            raise NotImplementedError

        if self.method == "DC3":
            utils.verify(args_DC3 is not None)
            self.args_DC3 = args_DC3

        self.cs = cs
        self.k = cs.k  # Dimension of the ambient space
        self.n = cs.n  # Dimension of the embedded space

        # Precompute for inverse distance to the frontier of Z along v_bar
        if cs.z0 is not None:
            print("\ncs.z0 is not None\n")
            D = cs.A_p / ((cs.b_p - cs.A_p @ cs.z0) @ np.ones((1, cs.n)))  # for linear

            all_P, all_q, all_r = utils.getAllPqrFromQcs(cs.qcs)
            all_M, all_s, all_c, all_d = utils.getAllMscdFromSocs(cs.socs)

            if cs.has_lmi_constraints:
                all_F = copy.deepcopy(cs.lmic.all_F)
                H = all_F[-1]
                for i in range(cs.lmic.dim()):
                    H += cs.y0[i, 0] * cs.lmic.all_F[i]
                Hinv = np.linalg.inv(H)
                mHinv = -Hinv
                L = np.linalg.cholesky(Hinv)  # Hinv = L @ L^T
                self.register_buffer("mHinv", torch.Tensor(mHinv))
                self.register_buffer("L", torch.Tensor(L))

            else:
                all_F = []

            # See https://discuss.pytorch.org/t/model-cuda-does-not-convert-all-variables-to-cuda/114733/9
            # and https://discuss.pytorch.org/t/keeping-constant-value-in-module-on-correct-device/10129
            self.register_buffer("D", torch.Tensor(D))
            self.register_buffer("all_P", torch.Tensor(np.array(all_P)))
            self.register_buffer("all_q", torch.Tensor(np.array(all_q)))
            self.register_buffer("all_r", torch.Tensor(np.array(all_r)))
            self.register_buffer("all_M", torch.Tensor(np.array(all_M)))
            self.register_buffer("all_s", torch.Tensor(np.array(all_s)))
            self.register_buffer("all_c", torch.Tensor(np.array(all_c)))
            self.register_buffer("all_d", torch.Tensor(np.array(all_d)))
            # self.register_buffer("all_F", torch.Tensor(np.array(all_F))) #This one dies (probably because out of memory) when all_F contains more than 7000 matrices 500x500 approx
            self.register_buffer("all_F", torch.Tensor(all_F))
            self.register_buffer("A_p", torch.Tensor(cs.A_p))
            self.register_buffer("b_p", torch.Tensor(cs.b_p))
            self.register_buffer("yp", torch.Tensor(cs.yp))
            self.register_buffer("NA_E", torch.Tensor(cs.NA_E))
            self.register_buffer("z0", torch.Tensor(cs.z0))
            self.register_buffer("y0", torch.Tensor(cs.y0))

        if self.method == "PP" or self.method == "UP":
            # Section 8.1.1 of https://web.stanford.edu/~boyd/cvxbook/bv_cvxbook.pdf
            self.z_projected = cp.Variable((self.n, 1))  # projected point
            self.z_to_be_projected = cp.Parameter((self.n, 1))  # original point
            constraints = self.cs.getConstraintsInSubspaceCvxpy(self.z_projected)

            # First option.
            objective = cp.Minimize(
                cp.sum_squares(self.z_projected - self.z_to_be_projected)
            )

            # Second option. Sometimes this may be preferred because of this: http://cvxr.com/cvx/doc/advanced.html#eliminating-quadratic-forms This may solve cases of ("Solver ecos returned status Infeasible" or "Solver SCS returned status Infeasible")
            # objective = cp.Minimize(cp.norm(self.z_projected - self.z_to_be_projected))

            self.prob_projection = cp.Problem(objective, constraints)

            assert self.prob_projection.is_dpp()
            self.proj_layer = CvxpyLayer(
                self.prob_projection,
                parameters=[self.z_to_be_projected],
                variables=[self.z_projected],
            )

            if self.cs.has_lmi_constraints:
                self.solver_projection = (
                    "SCS"  # slower, less accurate, supports LMI constraints
                )
            else:
                self.solver_projection = (
                    "ECOS"  # fast, accurate, does not support LMI constraints
                )

        if self.method == "RAYEN" or self.method == "RAYEN_old":
            # TODO
            self.m = cs.m  # Dimenstion of the constraint input variable x

            # This will only do online phase to handle non-fixed constraints
            self.ip_online = True if self.m > 0 else False
            # if self.ip_online:
            self.setupInteriorPointLayer()

            # Precompute to find roots of quadratic equation
            if cs.z0 is not None:
                if cs.has_quadratic_constraints:
                    all_delta = []
                    all_phi = []

                    for i in range(
                        self.all_P.shape[0]
                    ):  # for each of the quadratic constraints
                        P = self.all_P[i, :, :]
                        q = self.all_q[i, :, :]
                        r = self.all_r[i, :, :]
                        y0 = self.y0

                        sigma = 2 * (0.5 * y0.T @ P @ y0 + q.T @ y0 + r)
                        phi = -(y0.T @ P + q.T) / sigma
                        delta = (
                            (y0.T @ P + q.T).T @ (y0.T @ P + q.T)
                            - 4 * (0.5 * y0.T @ P @ y0 + q.T @ y0 + r) * 0.5 * P
                        ) / torch.square(sigma)

                        all_delta.append(delta)
                        all_phi.append(phi)

                    all_delta = torch.stack(all_delta)
                    all_phi = torch.stack(all_phi)

                    self.register_buffer("all_delta", all_delta)
                    self.register_buffer("all_phi", all_phi)

        if self.method == "Bar":
            print("Computing vertices and rays...")
            V, R = utils.H_to_V(cs.A_p, cs.b_p)
            self.register_buffer("V", torch.Tensor(V))
            self.register_buffer("R", torch.Tensor(R))
            self.num_vertices = self.V.shape[1]
            self.num_rays = self.R.shape[1]
            assert (self.num_vertices + self.num_rays) > 0
            print(f"Found {self.num_vertices} vertices and {self.num_rays} rays")

        if self.method == "DC3":
            A2_DC3, b2_DC3 = utils.removeRedundantEquationsFromEqualitySystem(
                cs.A_E, cs.b_E
            )

            self.register_buffer("A2_DC3", torch.Tensor(A2_DC3))
            self.register_buffer("b2_DC3", torch.Tensor(b2_DC3))
            self.register_buffer("A1_DC3", torch.Tensor(cs.A_I))
            self.register_buffer("b1_DC3", torch.Tensor(cs.b_I))

            # Constraints are now
            # A2_DC3 y = b2_DC3
            # A1_DC3 y <= b1_DC3

            self.neq_DC3 = self.A2_DC3.shape[0]

            #################################### Find partial_vars and other_vars

            if A2_DC3.shape[0] == 0:  # There are no equality constraints
                self.partial_vars = np.arange(self.k)
                self.other_vars = np.setdiff1d(np.arange(self.k), self.partial_vars)
            else:
                # This is a more efficient way to do https://github.com/locuslab/DC3/blob/35437af7f22390e4ed032d9eef90cc525764d26f/utils.py#L67
                # Here, we follow  https://stackoverflow.com/a/27907936
                (A2_DC3_rref, pivots_pos, row_exchanges) = utils.rref(A2_DC3)
                self.other_vars = [i[1] for i in pivots_pos]
                self.partial_vars = np.setdiff1d(np.arange(self.k), self.other_vars)

            #######################################################

            A2p = self.A2_DC3[:, self.partial_vars]
            A2o = self.A2_DC3[:, self.other_vars]

            # assert np.linalg.matrix_rank(A2_DC3) == np.linalg.matrix_rank(A2o) == A2o.shape[-1]

            A2oi = torch.inverse(A2o)

            ####################################################
            ####################################################

            A1p = self.A1_DC3[:, self.partial_vars]
            A1o = self.A1_DC3[:, self.other_vars]

            A1_effective = A1p - A1o @ (A2oi @ A2p)
            b1_effective = self.b1_DC3 - A1o @ A2oi @ self.b2_DC3

            all_P_effective = torch.Tensor(
                self.all_P.shape[0], len(self.partial_vars), len(self.partial_vars)
            )
            all_q_effective = torch.Tensor(
                self.all_q.shape[0], len(self.partial_vars), 1
            )
            all_r_effective = torch.Tensor(self.all_q.shape[0], 1, 1)

            self.register_buffer("A2oi", A2oi)
            self.register_buffer("A2p", A2p)
            self.register_buffer("A1_effective", A1_effective)
            self.register_buffer("b1_effective", b1_effective)

            ####################
            for i in range(
                self.all_P.shape[0]
            ):  # for each of the quadratic constraints
                P = self.all_P[i, :, :]
                q = self.all_q[i, :, :]
                r = self.all_r[i, :, :]

                Po = P[np.ix_(self.other_vars, self.other_vars)].view(
                    len(self.other_vars), len(self.other_vars)
                )
                Pp = P[np.ix_(self.partial_vars, self.partial_vars)].view(
                    len(self.partial_vars), len(self.partial_vars)
                )
                Pop = P[np.ix_(self.other_vars, self.partial_vars)].view(
                    len(self.other_vars), len(self.partial_vars)
                )

                qo = q[self.other_vars, 0:1]
                qp = q[self.partial_vars, 0:1]

                b2 = self.b2_DC3

                P_effective = 2 * (
                    -A2p.T @ A2oi.T @ Pop
                    + 0.5 * A2p.T @ A2oi.T @ Po @ A2oi @ A2p
                    + 0.5 * Pp
                )
                q_effective = (
                    b2.T @ A2oi.T @ Pop
                    + qp.T
                    - qo.T @ A2oi @ A2p
                    - b2.T @ A2oi.T @ Po @ A2oi @ A2p
                ).T
                r_effective = (
                    qo.T @ A2oi @ b2 + 0.5 * b2.T @ A2oi.T @ Po @ A2oi @ b2 + r
                )

                ###### QUICK CHECK
                # tmp=random.randint(1, 100) #number of elements in the batch
                # yp=torch.rand(tmp, len(self.partial_vars), 1)

                # y = torch.zeros((tmp, self.k, 1))
                # y[:, self.partial_vars, :] = yp
                # y[:, self.other_vars, :] = self.obtainyoFromypDC3(yp)

                # using_effective=utils.quadExpression(yp, P_effective, q_effective, r_effective)
                # using_original=utils.quadExpression(y, P, q, r)

                # assert torch.allclose(using_effective, using_original, atol=1e-05)

                ###################

                all_P_effective[i, :, :] = P_effective
                all_q_effective[i, :, :] = q_effective
                all_r_effective[i, :, :] = r_effective

            self.register_buffer("all_P_effective", all_P_effective)
            self.register_buffer("all_q_effective", all_q_effective)
            self.register_buffer("all_r_effective", all_r_effective)

            ####################################################
            ####################################################

        if self.method == "RAYEN_old":
            self.forwardForMethod = self.forwardForRAYENOld
            self.dim_after_map = self.n + 1
        elif self.method == "RAYEN":
            self.forwardForMethod = self.forwardForRAYEN
            self.dim_after_map = self.n
        elif self.method == "UU":
            self.forwardForMethod = self.forwardForUU
            self.dim_after_map = self.k
        elif self.method == "Bar":
            self.forwardForMethod = self.forwardForBar
            self.dim_after_map = self.num_vertices + self.num_rays
        elif self.method == "PP":
            self.forwardForMethod = self.forwardForPP
            self.dim_after_map = self.n
        elif self.method == "UP":
            self.forwardForMethod = self.forwardForUP
            self.dim_after_map = self.n
        elif self.method == "DC3":
            self.forwardForMethod = self.forwardForDC3
            self.dim_after_map = self.k - self.neq_DC3
            assert self.dim_after_map == self.n
        else:
            raise NotImplementedError

        if create_map:
            utils.verify(input_dim is not None, "input_dim needs to be provided")
            self.mapper = nn.Linear(input_dim, self.dim_after_map)
        else:
            self.mapper = nn.Sequential()
            # Mapper does nothing

    # Constraint Completion in DC3, obtaining y_other from y_partial
    def obtainyoFromypDC3(self, yp):
        return self.A2oi @ (self.b2_DC3 - self.A2p @ yp)

    def forwardForDC3(self, q):
        #### Complete partial
        y = torch.zeros((q.shape[0], self.k, 1), device=q.device)
        y[:, self.partial_vars, :] = q
        y[:, self.other_vars, :] = self.obtainyoFromypDC3(q)

        #### Grad steps all

        y_new = y
        step_index = 0
        old_y_step = 0

        if self.training:
            max_steps = self.args_DC3[
                "max_steps_training"
            ]  # This is called corrTrainSteps in DC3 original code
        else:
            max_steps = self.args_DC3[
                "max_steps_testing"
            ]  # float("inf") #This is called corrTestMaxSteps in DC3 original code

        while True:
            ################################################
            ################################################ COMPUTE y_step

            yp = y_new[:, self.partial_vars, :]
            ypT = torch.transpose(yp, 1, 2)

            grad = (
                2
                * self.A1_effective.T
                @ torch.relu(self.A1_effective @ yp - self.b1_effective)
            )

            for i in range(
                self.all_P_effective.shape[0]
            ):  # for each of the quadratic constraints
                P_effective = self.all_P_effective[i, :, :]
                q_effective = self.all_q_effective[i, :, :]
                r_effective = self.all_r_effective[i, :, :]

                tmp1 = P_effective @ yp + q_effective
                tmp2 = torch.relu(
                    utils.quadExpression(yp, P_effective, q_effective, r_effective)
                )

                grad += 2 * tmp1 @ tmp2  # The 2 is because of the squared norm

            y_step = torch.zeros_like(y)
            y_step[:, self.partial_vars, :] = grad
            y_step[:, self.other_vars, :] = -self.A2oi @ self.A2p @ grad
            ################################################
            ################################################

            new_y_step = (
                self.args_DC3["lr"] * y_step + self.args_DC3["momentum"] * old_y_step
            )
            y_new = y_new - new_y_step
            old_y_step = new_y_step
            step_index += 1

            ################################################
            ################################################ COMPUTE current violation
            stacked = self.A1_DC3 @ y_new - self.b1_DC3
            for i in range(
                self.all_P.shape[0]
            ):  # for each of the quadratic constraints
                stacked = torch.cat(
                    (
                        stacked,
                        utils.quadExpression(
                            y_new,
                            self.all_P[i, :, :],
                            self.all_q[i, :, :],
                            self.all_r[i, :, :],
                        ),
                    ),
                    dim=1,
                )
            violation = torch.max(torch.relu(stacked))
            ################################################
            ################################################

            converged_ineq = violation < self.args_DC3["eps_converge"]
            max_iter_reached = step_index >= max_steps

            if max_iter_reached:
                break

            if converged_ineq:
                break

        return y_new

    def solveSecondOrderEq(self, a, b, c, is_quad_constraint):
        discriminant = torch.square(b) - 4 * (a) * (c)

        assert torch.all(
            discriminant >= 0
        ), f"Smallest element is {torch.min(discriminant)}"
        sol1 = torch.div(
            -(b) - torch.sqrt(discriminant), 2 * a
        )  # note that for quad constraints the positive solution has the minus: (... - sqrt(...))/(...)
        if is_quad_constraint:
            return sol1
        else:
            sol2 = torch.div(-(b) + torch.sqrt(discriminant), 2 * a)
            return torch.relu(torch.maximum(sol1, sol2))

    def computeKappa(self, v_bar):
        kappa = torch.relu(torch.max(self.D @ v_bar, dim=1, keepdim=True).values)

        if len(self.all_P) > 0 or len(self.all_M) > 0 or len(self.all_F) > 0:
            rho = self.NA_E @ v_bar
            rhoT = torch.transpose(rho, dim0=1, dim1=2)
            all_kappas_positives = torch.empty(
                (v_bar.shape[0], 0, 1), device=v_bar.device
            )

            for i in range(
                self.all_P.shape[0]
            ):  # for each of the quadratic constraints
                # FIRST WAY (slower, easier to understand)
                # P=self.all_P[i,:,:]
                # q=self.all_q[i,:,:]
                # r=self.all_r[i,:,:]

                # c_prime=0.5*rhoT@P@rho;
                # b_prime=(self.y0.T@P+ q.T)@rho;
                # a_prime=(0.5*self.y0.T@P@self.y0 + q.T@self.y0 +r)

                # kappa_positive_i_first_way=self.solveSecondOrderEq(a_prime, b_prime, c_prime, True)

                # SECOND WAY (faster)
                kappa_positive_i = self.all_phi[i, :, :] @ rho + torch.sqrt(
                    rhoT @ self.all_delta[i, :, :] @ rho
                )

                # assert torch.allclose(kappa_positive_i,kappa_positive_i_first_way, atol=1e-06), f"{torch.max(torch.abs(kappa_positive_i-kappa_positive_i_first_way))}"

                assert torch.all(
                    kappa_positive_i >= 0
                ), f"Smallest element is {kappa_positive_i}"  # If not, then Z may not be feasible (note that z0 is in the interior of Z)
                all_kappas_positives = torch.cat(
                    (all_kappas_positives, kappa_positive_i), dim=1
                )

            for i in range(self.all_M.shape[0]):  # for each of the SOC constraints
                M = self.all_M[i, :, :]
                s = self.all_s[i, :, :]
                c = self.all_c[i, :, :]
                d = self.all_d[i, :, :]

                beta = M @ self.y0 + s
                tau = c.T @ self.y0 + d

                c_prime = rhoT @ M.T @ M @ rho - torch.square(c.T @ rho)
                b_prime = 2 * rhoT @ M.T @ beta - 2 * (c.T @ rho) @ tau
                a_prime = beta.T @ beta - torch.square(tau)

                kappa_positive_i = self.solveSecondOrderEq(
                    a_prime, b_prime, c_prime, False
                )

                assert torch.all(
                    kappa_positive_i >= 0
                )  # If not, then either the feasible set is infeasible (note that z0 is inside the feasible set)
                all_kappas_positives = torch.cat(
                    (all_kappas_positives, kappa_positive_i), dim=1
                )

            if len(self.all_F) > 0:  # If there are LMI constraints:
                ############# OBTAIN S
                # First option (much slower)
                # S=self.all_F[0,:,:]*rho[:,0:(0+1),0:1]
                # for i in range(1,len(self.all_F)-1):
                # 	#See https://discuss.pytorch.org/t/scalar-matrix-multiplication-for-a-tensor-and-an-array-of-scalars/100174/2
                # 	S += self.all_F[i,:,:]*rho[:,i:(i+1),0:1]

                # Second option (much faster)
                S = torch.einsum(
                    "ajk,ial->ijk", [self.all_F[0:-1, :, :], rho]
                )  # See the tutorial https://rockt.github.io/2018/04/30/einsum

                ############# COMPUTE THE EIGENVALUES

                ## Option 1: (compute whole spectrum of the matrix, using the non-symmetric matrix self.mHinv@S)
                # eigenvalues = torch.unsqueeze(torch.linalg.eigvals(self.mHinv@S),2) #Note that mHinv@M is not symmetric but always have real eigenvalues
                # assert (torch.all(torch.isreal(eigenvalues)))
                # largest_eigenvalue = torch.max(eigenvalues.real, dim=1, keepdim=True).values

                LTmSL = self.L.T @ (-S) @ self.L  # This matrix is symmetric

                ## Option 2: (compute whole spectrum of the matrix, using the symmetric matrix LTmSL). Much faster than Option 1
                eigenvalues = torch.unsqueeze(
                    torch.linalg.eigvalsh(LTmSL), 2
                )  # Note that L^T (-S) L is a symmetric matrix
                largest_eigenvalue = torch.max(eigenvalues, dim=1, keepdim=True).values

                ## Option 3: Use LOBPCG with A=LTmSL and B=I. The advantage of this method is that only the largest eigenvalue is computed. But, empirically, this option is faster than option 2 only for very big matrices (>1000x1000)
                # guess_lobpcg=torch.rand(1, H.shape[0], 1);
                # size_batch=v_bar.shape[0]
                # largest_eigenvalue, _ = torch.lobpcg(A=LTmSL, k=1, B=None, niter=-1) #, X=guess_lobpcg.expand(size_batch, -1, -1)
                # largest_eigenvalue=torch.unsqueeze(largest_eigenvalue, 1)

                ## Option 4: Use power iteration to compute the largest eigenvalue. Often times is slower than just computing the whole spectrum, and sometimes it does not converge
                # guess_v = torch.nn.functional.normalize(torch.rand(S.shape[1],1), dim=0)
                # largest_eigenvalue=utils.findLargestEigenvalueUsingPowerIteration(self.mHinv@S, guess_v)

                ## Option 5: Use LOBPCG with A=-S and B=H. There are two problems though:
                # --> This issue: https://github.com/pytorch/pytorch/issues/101075
                # --> Backward is not implemented for B!=I, see: https://github.com/pytorch/pytorch/blob/d54fcd571af48685b0699f6ac1e31b6871d0d768/torch/_lobpcg.py#L329

                ## Option 6: Use https://github.com/rfeinman/Torch-ARPACK with LTmSL. The problem is that backward() is not implemented yet

                ## Option 7: Use https://github.com/buwantaiji/DominantSparseEigenAD. But it does not have support for batched matrices, see https://github.com/buwantaiji/DominantSparseEigenAD/issues/1

                kappa_positive_i = torch.relu(largest_eigenvalue)

                all_kappas_positives = torch.cat(
                    (all_kappas_positives, kappa_positive_i), dim=1
                )

            kappa_nonlinear_constraints = torch.max(
                all_kappas_positives, dim=1, keepdim=True
            ).values
            kappa = torch.maximum(kappa, kappa_nonlinear_constraints)

        assert torch.all(kappa >= 0)

        return kappa

    # TODO
    # Setup interior point problem beforehand
    def setupInteriorPointLayer(self):
        self.ip_epsilon = cp.Variable()
        self.ip_z0 = cp.Variable((self.n, 1))
        # self.ip_constraints = cp.Parameter((1, 1))  # original point ???
        self.params = cp.Parameter((1, 1))
        self.ip_constraints = self.cs.getConstraintsInSubspaceCvxpy(
            self.ip_z0, self.ip_epsilon
        )
        self.ip_constraints.append(self.ip_epsilon >= 0)
        self.ip_constraints.append(
            self.ip_epsilon <= 0.5
        )  # This constraint is needed for the case where the set is unbounded. Any positive value is valid

        objective = cp.Minimize(-self.ip_epsilon + self.params)
        self.ip_prob = cp.Problem(objective, self.ip_constraints)

        assert self.ip_prob.is_dpp()
        self.ip_layer = CvxpyLayer(
            self.ip_prob,
            parameters=[self.params],
            variables=[self.ip_z0],
        )

        if self.cs.has_lmi_constraints:
            self.ip_solver = "SCS"  # slower, less accurate, supports LMI constraints
        else:
            self.ip_solver = "ECOS"  # fast, accurate, does not support LMI constraints
        return True

    # TODO
    # Function to compute z0
    def solveInteriorPoint(self):
        # Update constraint
        # self.ip_constraints = self.cs.getConstraintsInSubspaceCvxpy(
        #     self.ip_z0, self.ip_epsilon
        # )
        # self.ip_prob.solve(verbose=False, solver=self.ip_solver)
        # ip_z0 = self.ip_z0.value

        (ip_z0,) = self.ip_layer(
            torch.zeros(1, 1, 1),
            solver_args={"solve_method": self.ip_solver},
        )  # "max_iters": 10000

        # if ip_prob.status != "optimal" and ip_prob.status != "optimal_inaccurate":
        #     raise Exception(f"Value is not optimal, prob_status={ip_prob.status}")

        # utils.verify(
        #     epsilon.value > 1e-8
        # )  # If not, there are no strictly feasible points in the subspace

        return ip_z0

    # TODO
    # Function to recompute and register params
    def updateAndRegisterParams(self):
        # Precompute for inverse distance to the frontier of Z along v_bar
        D = self.cs.A_p / (
            (self.cs.b_p - self.cs.A_p @ self.cs.z0) @ np.ones((1, self.n))
        )  # for linear

        all_P, all_q, all_r = utils.getAllPqrFromQcs(self.cs.qcs)
        all_M, all_s, all_c, all_d = utils.getAllMscdFromSocs(self.cs.socs)

        if self.cs.has_lmi_constraints:
            all_F = copy.deepcopy(self.cs.lmic.all_F)
            H = all_F[-1]
            for i in range(self.cs.lmic.dim()):
                H += self.cs.y0[i, 0] * self.cs.lmic.all_F[i]
            Hinv = np.linalg.inv(H)
            mHinv = -Hinv
            L = np.linalg.cholesky(Hinv)  # Hinv = L @ L^T
            self.register_buffer("mHinv", torch.Tensor(mHinv))
            self.register_buffer("L", torch.Tensor(L))

        else:
            all_F = []

        # See https://discuss.pytorch.org/t/model-cuda-does-not-convert-all-variables-to-cuda/114733/9
        # and https://discuss.pytorch.org/t/keeping-constant-value-in-module-on-correct-device/10129
        self.register_buffer("D", torch.Tensor(D))
        self.register_buffer("all_P", torch.Tensor(np.array(all_P)))
        self.register_buffer("all_q", torch.Tensor(np.array(all_q)))
        self.register_buffer("all_r", torch.Tensor(np.array(all_r)))
        self.register_buffer("all_M", torch.Tensor(np.array(all_M)))
        self.register_buffer("all_s", torch.Tensor(np.array(all_s)))
        self.register_buffer("all_c", torch.Tensor(np.array(all_c)))
        self.register_buffer("all_d", torch.Tensor(np.array(all_d)))
        # self.register_buffer("all_F", torch.Tensor(np.array(all_F))) #This one dies (probably because out of memory) when all_F contains more than 7000 matrices 500x500 approx
        self.register_buffer("all_F", torch.Tensor(all_F))
        self.register_buffer("A_p", torch.Tensor(self.cs.A_p))
        self.register_buffer("b_p", torch.Tensor(self.cs.b_p))
        self.register_buffer("yp", torch.Tensor(self.cs.yp))
        self.register_buffer("NA_E", torch.Tensor(self.cs.NA_E))
        # self.register_buffer("z0", torch.Tensor(self.z0))
        # self.register_buffer("y0", torch.Tensor(self.y0))

        self.y0 = self.gety0()

        # Precompute to find roots of quadratic equation
        if self.cs.has_quadratic_constraints:
            all_delta = []
            all_phi = []

            for i in range(
                self.all_P.shape[0]
            ):  # for each of the quadratic constraints
                P = self.all_P[i, :, :]
                q = self.all_q[i, :, :]
                r = self.all_r[i, :, :]
                y0 = self.y0

                sigma = 2 * (0.5 * y0.T @ P @ y0 + q.T @ y0 + r)
                phi = -(y0.T @ P + q.T) / sigma
                delta = (
                    (y0.T @ P + q.T).T @ (y0.T @ P + q.T)
                    - 4 * (0.5 * y0.T @ P @ y0 + q.T @ y0 + r) * 0.5 * P
                ) / torch.square(sigma)

                all_delta.append(delta)
                all_phi.append(phi)

            all_delta = torch.stack(all_delta)
            all_phi = torch.stack(all_phi)

            self.register_buffer("all_delta", all_delta)
            self.register_buffer("all_phi", all_phi)

        return True

    # TODO
    # Forward pass for RAYEN
    def forwardForRAYEN(self, q):
        # nn.Module forward method only accepts a single input tensor
        v = q[:, 0 : self.n, 0:1]

        # if self.ip_online:
        #     print("Online Interior Point")
        #     x = q[
        #         :, self.n : self.n + self.m, 0:1
        #     ]  # x in Rm is part of the input tensor

        #     # Update current constraint set based on input x, update Ap, bp, NA_E
        #     self.cs.updateConstraintSet(x)

        #     # Check current y0 if it is still interior point
        #     if self.cs.isInteriorPoint(self.y0):
        #         # Just update z0 and related linear params
        #         pass
        #     else:
        #         # Solve interior point
        #         self.z0 = self.solveInteriorPoint()
        #         self.y0 = self.gety0()

        #         # Update and register all necessary parameters
        #         self.updateAndRegisterParams()

        if 1:
            print("Online Interior Point")
            self.z0 = self.solveInteriorPoint()[0]
            self.cs.z0 = self.z0.detach().cpu().numpy()
            # print(self.z0)
            # print(self.y0)
            self.updateAndRegisterParams()
            self.cs.y0 = self.y0.detach().cpu().numpy()

        v_bar = torch.nn.functional.normalize(v, dim=1)
        kappa = self.computeKappa(v_bar)
        norm_v = torch.linalg.vector_norm(v, dim=(1, 2), keepdim=True)
        alpha = torch.minimum(1 / kappa, norm_v)
        return self.getyFromz(self.z0 + alpha * v_bar)

    def forwardForRAYENOld(self, q):
        v = q[:, 0 : self.n, 0:1]
        v_bar = torch.nn.functional.normalize(v, dim=1)
        kappa = self.computeKappa(v_bar)
        beta = q[:, self.n : (self.n + 1), 0:1]
        alpha = 1 / (torch.exp(beta) + kappa)
        return self.getyFromz(self.z0 + alpha * v_bar)

    def forwardForUU(self, q):
        return q

    def forwardForBar(self, q):
        tmp1 = q[:, 0 : self.num_vertices, 0:1]  # 0:1 to keep the dimension.
        tmp2 = q[
            :, self.num_vertices : (self.num_vertices + self.num_rays), 0:1
        ]  # 0:1 to keep the dimension.

        lambdas = nn.functional.softmax(tmp1, dim=1)
        mus = torch.abs(tmp2)

        return self.getyFromz(self.V @ lambdas + self.R @ mus)

    def project(self, q):
        # If you use ECOS, you can set solver_args={'eps': 1e-6} (or smaller) for better solutions, see https://github.com/cvxpy/cvxpy/issues/880#issuecomment-557278620
        (z,) = self.proj_layer(
            q, solver_args={"solve_method": self.solver_projection}
        )  # "max_iters": 10000
        return z

    def forwardForPP(self, q):
        z = self.project(q)
        return self.getyFromz(z)

    def forwardForUP(self, q):
        if self.training == False:
            z = self.project(q)
        else:
            z = q

        return self.getyFromz(z)

    def getDimAfterMap(self):
        return self.dim_after_map

    def gety0(self):
        return self.getyFromz(self.z0)

    def getyFromz(self, z):
        y = self.NA_E @ z + self.yp
        return y

    def getzFromy(self, y):
        z = self.NA_E.T @ (y - self.yp)
        return z

    def forward(self, x):
        ##################  MAPPER LAYER ####################
        # nsib denotes the number of samples in the batch
        # x has dimensions [nsib, numel_input_mapper, 1]. nsib is the number of samples in the batch (i.e., x.shape[0]=x.shape[0])
        q = self.mapper(
            x.view(x.size(0), -1)
        )  # After this, q has dimensions [nsib, numel_output_mapper]
        q = torch.unsqueeze(
            q, dim=2
        )  # After this, q has dimensions [nsib, numel_output_mapper, 1]
        ####################################################

        y = self.forwardForMethod(q)

        assert (
            torch.isnan(y).any()
        ) == False, f"If you are using DC3, try reducing args_DC3[lr]. Right now it's {self.args_DC3['lr']}"

        return y
