# Import packages.
import cvxpy as cp
import numpy as np
from numpy import linalg as LA
import utils
import torch
import scipy


A = torch.Tensor(
    [[[2.0, -12.0], [1.0, -5.0]], [[-7.0, 0.0], [0.0, 5.0]], [[-2.0, 0.0], [0.0, 6.0]]]
)

guess_v = torch.nn.functional.normalize(torch.rand(A.shape[1], 1), dim=0)

lamb = utils.findLargestEigenvalue(A, guess_v)

L, V = torch.linalg.eig(A)
print(L)

print(f"Found lambda={lamb}")

exit()

dim = 3

for trial in range(200):
    # Generate a random SDP.
    tmp = np.random.rand(dim, dim)
    # tmp = np.random.randint(0, 20, size=(dim, dim))
    H = np.dot(tmp, tmp.transpose()) + np.eye(dim)  # H is psd by construction

    tmp = np.random.rand(dim, dim)
    # tmp = np.random.randint(0, 20, size=(dim, dim))
    S = (tmp + tmp.T) / 2.0  # M is symmetric by construction

    # tmp = np.random.rand(dim, dim)
    # M = np.dot(tmp, tmp.transpose()) #H is psd by construction

    Hinv = np.linalg.inv(H)

    print("\n\n-------")
    print("-------")

    print(f"H={H}\n")
    print(f"S={S}\n")

    # kappa_opt = cp.Variable()
    # constraints = [(kappa_opt*H+S) >> 0, kappa_opt>=0]

    # prob = cp.Problem(cp.Minimize(kappa_opt), constraints)
    # prob.solve(solver=cp.SCS, verbose=True, eps=1e-7)

    # if(prob.status=='unbounded'):
    #     utils.printInBoldRed("Unbounded!!!!") #When kappa is the decision variable, there is no way the result is unbounded
    # else:
    #     kappa_opt=kappa_opt.value
    #     utils.printInBoldBlue(f"kappa_opt={kappa_opt}")

    X = np.random.rand(S.shape[0], 1)

    #### USING lobpcg
    kappa_lobpcg, _ = torch.lobpcg(
        A=torch.from_numpy(-S),
        k=1,
        B=torch.from_numpy(H),
        niter=-1,
        X=torch.from_numpy(X),
    )
    # kappa_lobpcg = torch.relu(kappa_lobpcg).item()
    kappa_lobpcg = kappa_lobpcg.item()
    utils.printInBoldBlue(f"kappa_lobpcg={kappa_lobpcg}")

    # #### USING Scipy
    # kappa_scipy, _ =scipy.sparse.linalg.lobpcg(A=-S, B=H, X=X, maxiter=-1)
    # kappa_scipy=kappa_scipy[0]
    # utils.printInBoldBlue(f"kappa_scipy={kappa_scipy}")

    #### USING normal eigendecomposition
    all_kappas, _ = np.linalg.eig(-Hinv @ S)
    # Be careful because Hinv@M is NOT symmetric (Hinv and M is)
    print(f"all_kappas={all_kappas}")
    # kappa_eig=np.maximum(np.max(all_kappas),0.0)
    kappa_eig = np.max(all_kappas)
    utils.printInBoldBlue(f"kappa_eig={kappa_eig}")

    tol = 1e-6
    assert abs(kappa_lobpcg - kappa_eig) < tol
    # assert abs(kappa_scipy-kappa_eig)<tol
    # assert abs(kappa_opt-kappa_eig)<tol #This one sometimes fails due to inaccuracies in the optimization


# LPBPCG algorithm is not applicable when the number of A rows (=2) is smaller than 3 x the number of requested eigenpairs (=1)


# This should be zero
# print(np.linalg.det(H+lam.value*M))

# print(np.linalg.det(Minv@H+lam.value*np.eye(2)))

# for i in range(all_kappas.shape[0]):
#     tmp=all_kappas[i]

#     # print(np.linalg.det(-Hinv@M-tmp*np.eye(dim)))
#     print(f"tmp={tmp}")
#     eigenvals,_=np.linalg.eig(tmp*H+S)
#     print(eigenvals)


# print("-------")
# print("Eigen Decomposition of H")

# wH, vH = LA.eig(H)
# print(f"Eigenvalues={wH}")
# print(f"Eigenvectors=\n{vH}")

# print("-------")
# print("Eigen Decomposition of M")

# wM, vM = LA.eig(M)
# print(f"Eigenvalues={wM}")
# print(f"Eigenvectors=\n{vM}\n")

# for i in range(wH.shape[0]):
#     for j in range(wM.shape[0]):
#         print(wH[i]/wM[j])
#         print(wM[j]/wH[i])
#         print(wH[i]*wM[j])

# beta = cp.Variable()
# constraints = [H >> beta*M]

# prob = cp.Problem(cp.Minimize(beta), constraints)
# prob.solve()
# print(f"beta={beta.value}")

# Hinv=np.linalg.inv(H)
# constraints = [(np.eye(2)+lam*Hinv@M) >> 0]
# prob = cp.Problem(cp.Maximize(lam), constraints)
# prob.solve()
# print(f"lam={lam.value}")

# L = np.linalg.cholesky(H)
# # print(L@L.T-H) #good

# Y=np.linalg.inv(L)@M@(np.linalg.inv(L).T)

# constraints = [(Y+lam*np.eye(2)) >> 0]

# prob = cp.Problem(cp.Maximize(lam), constraints)
# prob.solve()
# print(f"lam={lam.value}")


# Inspired by https://github.com/rfeinman/Torch-ARPACK/blob/master/arpack/power_iteration.py
# def power_iteration(A, tol=1e-20, max_iter=10000, eps=1e-12, check_freq=2):

#     v = torch.nn.functional.normalize(torch.rand(A.shape[1],1), dim=0)

#     n_iter = 0
#     converging = torch.ones(A.shape[0], dtype=torch.bool)# [True True True ...]
#     print(converging)
#     while n_iter < max_iter:
#         n_iter += 1
#         if(n_iter==1):
#             u = torch.nn.functional.normalize(A@v, dim=1)
#             v = u
#             continue

#         else:

#             print(f"converging before={converging}")

#             u[converging,:,:] = torch.nn.functional.normalize(A[converging,:,:]@v[converging,:,:], dim=1)

#             distance=torch.abs(1 - torch.abs(  torch.transpose(v,1,2)@u ))
#             print(f"distance={distance}")

#             v[converging,:,:] = u[converging,:,:]

#             converging = (distance>tol).flatten()

#             print(f"converging after={converging}")


#             if (torch.all(converging == False)): #All of them converged
#                 break

#     else:
#         warnings.warn('power iteration did not converge')


#     lamb =  torch.transpose(v,1,2)@A@v #torch.dot(v, torch.mv(A, v))

#     return lamb
